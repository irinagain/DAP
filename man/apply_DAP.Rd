% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/apply_DAP.R
\name{apply_DAP}
\alias{apply_DAP}
\title{Apply DAP for binary calssification}
\usage{
apply_DAP(xtrain, ytrain, xtest, ytest = NULL, lambda_seq = NULL,
  n_lambda = 50, maxmin_ratio = 0.1, nfolds = 5, eps = 1e-04,
  m_max = 10000, myseed = 1001, prior = TRUE)
}
\arguments{
\item{xtrain}{Total training data.}

\item{ytrain}{Total training label, either "1" or "2".}

\item{xtest}{Test data.}

\item{ytest}{Test data label, either "1" or "2". Once it's provided,
this function will return a misclassification error rate on the test set;
otherwise, the function will return predicted labels for the test set. 
Default is NULL.}

\item{lambda_seq}{A sequence of tunning parameter, lambda. Deafult is NULL.}

\item{n_lambda}{Length of lambda_seq, used for generating lambda_seq if it's
NULL. Default is 50.}

\item{maxmin_ratio}{A ratio to control the minimum lambda in lambda_seq.
Default is 0.1.}

\item{nfolds}{Set folds number for cross-validation. Default is 5.}

\item{eps}{Convergence threshold for block-coordinate decent
algorithm. Each block-coordinate decent algorithm loop continuoues
until the maximum iteration number exceeds \code{maxiter} or the
maximum element-wise change in \eqn{V} is less than \code{eps}.
Default is 1e-4.}

\item{m_max}{Maximum number of iterations. Default is 10000.}

\item{myseed}{Seed for random spliting the data set into traininf
and testing. Default seed is 1001.}

\item{prior}{If "TRUE", the proportions for the training set will
be used to adjust the classification rule. Default is "TRUE".}
}
\value{
A list as below.
       \item{error}{ Misclassification error rate if ytest is provided.}
       \item{ypred}{predicted label on the test set if ytest is NULL.}
       \item{features}{Number of selected features.}
       \item{feature_id}{Index of selected features.}
}
\description{
Apply sparse quadratic classification rules via linear dimension
reduction (projection matrix). Meanwhile, variable selection via
group lasso will be implemented. Can deal with high-dimensional
binary classification problem.
}
\details{
If no feature is selected by DAP, no matter ytest is NULL or 
provided, the function will return error = 0.5 and no ypred. In this case,
the classifier is no better than randomly guessing.
}
\examples{
## This is an example for apply_DAP

## Generate data
n1 = n2 = 50
n_test = 50
p = 100
mu1 = rep(0, p)
mu2 = rep(3, p)
Sigma1 = diag(p)
Sigma2 = 0.5* diag(p)

## Build training data and test data
x1 = MASS::mvrnorm(n = n1, mu = mu1, Sigma = Sigma1)
x2 = MASS::mvrnorm(n = n2, mu = mu2, Sigma = Sigma2)
y1 = rep(1, n1)
y2 = rep(2, n2)
xtrain <- rbind(x1, x2)
x1_test = MASS::mvrnorm(n = n_test, mu = mu1, Sigma = Sigma1)
x2_test = MASS::mvrnorm(n = n_test, mu = mu2, Sigma = Sigma2)
xtest <- rbind(x1_test, x2_test)
ytrain <- c(rep(1, n1), rep(2, n2))
ytest <- c(rep(1, n_test), rep(2, n_test))

## Apply DAP

# Given ytest, the function will return a miclassification error rate.
ClassificationError = apply_DAP(xtrain, ytrain, xtest, ytest)

# Without ytest, the function will return predictions.
Ypredict = apply_DAP(xtrain, ytrain, xtest)
}
